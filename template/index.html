<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Image Captioning</title>
    <link
      href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;900&display=swap"
      rel="stylesheet"
    />
    <script src="https://cdn.jsdelivr.net/npm/canvas-confetti@1.6.0/dist/confetti.browser.min.js"></script>
    <style>
      body {
        font-family: "Inter", sans-serif;
        background-image: linear-gradient(to bottom right, #c3dafe, #e9d8fd),
          url("https://www.transparenttextures.com/patterns/cubes.png");
        background-blend-mode: overlay;
        background-size: cover;
      }

      .fade-up {
        animation: fadeUp 0.8s ease-out both;
      }

      @keyframes fadeUp {
        0% {
          opacity: 0;
          transform: translateY(20px);
        }
        100% {
          opacity: 1;
          transform: translateY(0);
        }
      }
    </style>
  </head>

  <body class="min-h-screen flex flex-col items-center justify-center p-4 sm:p-6 space-y-10">
    
    <div
      id="loadingOverlay"
      class="fixed inset-0 bg-white bg-opacity-80 flex items-center justify-center z-50 hidden"
    >
      <div
        class="animate-spin rounded-full h-16 w-16 border-t-4 border-b-4 border-indigo-500"
      ></div>
    </div>

    <div
      class="bg-white shadow-2xl rounded-3xl w-full max-w-4xl overflow-hidden border border-indigo-100 fade-up"
    >
      <div
        class="bg-gradient-to-r from-indigo-600 to-purple-600 text-white text-center py-6 px-4 sm:py-8 sm:px-6"
      >
        <h1
          class="text-3xl sm:text-4xl md:text-5xl font-extrabold tracking-tight"
        >
          Image Captioning App
        </h1>
        <p class="mt-2 sm:mt-3 text-lg sm:text-xl font-light">
          Let deep learning describe what it sees
        </p>
      </div>

      <div class="p-6 sm:p-10">
        <form
          id="uploadForm"
          class="flex flex-col items-center space-y-6 sm:space-y-8"
        >
          <div
            class="flex flex-col sm:flex-row items-center justify-center gap-6 w-full"
          >
            <label
              for="imageInput"
              class="relative cursor-pointer w-full sm:w-64 h-44 border-4 border-dashed border-indigo-300 flex items-center justify-center rounded-2xl bg-indigo-50 hover:bg-indigo-100 transition overflow-hidden"
            >
              <span
                id="uploadText"
                class="text-indigo-600 font-semibold text-base sm:text-lg z-10"
                >üì∑ Upload Image</span
              >
              <input
                type="file"
                id="imageInput"
                accept="image/*"
                class="hidden"
              />
            </label>

            <img
              id="previewImage"
              class="w-40 sm:w-52 h-auto rounded-2xl shadow-xl hidden ring-2 ring-indigo-300 object-contain"
            />
          </div>

          <button
            type="submit"
            class="bg-gradient-to-r from-indigo-500 to-purple-500 hover:from-indigo-600 hover:to-purple-600 text-white font-semibold py-2 px-5 rounded-2xl text-sm sm:text-base shadow-lg transition-all duration-300"
          >
            üöÄ Generate
          </button>
        </form>

        <div id="captionResult" class="mt-8 sm:mt-10 text-center hidden">
          <p class="text-base sm:text-lg text-gray-500 mb-1 sm:mb-2">
            üìù Generated Caption
          </p>
          <p
            id="captionText"
            class="text-sm sm:text-2xl text-gray-800 bg-gray-100 px-4 py-3 rounded-xl inline-block shadow-md max-w-xs sm:max-w-md mx-auto"
          ></p>
        </div>
      </div>
    </div>

    <!-- üìò Model Theory Section -->
    <div
      class="bg-white rounded-3xl shadow-2xl max-w-4xl w-full px-8 py-10 fade-up border border-indigo-100"
    >
      <section class="max-w-4xl mx-auto px-4 sm:px-6 py-12">
      <h2 class="text-3xl sm:text-4xl font-extrabold text-indigo-700 mb-6 border-b-4 border-purple-300 pb-2">
         How the Image Captioning Model Works
      </h2>

      <div class="space-y-6 text-lg leading-relaxed">
        <p>
          This image captioning project is based on a deep learning architecture that blends visual understanding with language generation. The core idea is to interpret the contents of an image and then generate a natural-sounding sentence that describes it.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">1. Dataset & Feature Extraction</h3>
        <p>
          The system begins with a dataset that contains images paired with human-written captions. Each image is processed using a pre-trained convolutional neural network (CNN), typically VGG16 , to extract high-level features. These features serve as a numeric representation of the image's content.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">2. Text Tokenization</h3>
        <p>
          The captions from the dataset are broken into tokens (individual words) and converted into sequences of integers. A tokenizer maps words to numbers so they can be fed into the model.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">3. Model Architecture</h3>
        <p>
          The model uses a two-branch architecture:
        </p>
        <ul class="list-disc list-inside ml-4">
          <li><strong>Image Branch:</strong> This processes the image features using dense layers to reduce dimensionality.</li>
          <li><strong>Caption Branch:</strong> This processes the input caption sequence using embedding and LSTM layers to learn language patterns.</li>
        </ul>
        <p>
          These two branches are merged and passed through additional LSTM and dense layers to predict the next word in the caption, one step at a time.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">4. Training the Model</h3>
        <p>
          During training, the model is given an image and a partial caption and learns to predict the next word. Over time, it learns the correct sequence of words for each image, using teacher forcing and categorical loss functions to improve accuracy.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">5. Caption Generation</h3>
        <p>
          When generating a caption, the model starts with a special start token and generates one word at a time, feeding each word back into the LSTM until it generates an end token or reaches a maximum length.
        </p>

        <h3 class="text-2xl font-semibold text-purple-700 mt-8">6. Limitations</h3>
        <p>
          This notebook uses a relatively small dataset and lightweight architecture for faster training and lower memory usage. While it can generate reasonable captions, it might not handle complex scenes as well as large-scale models like those used by big tech companies.
        </p>

        <p class="mt-8 text-indigo-800 italic">
          In summary, the model combines visual feature extraction with sequence prediction to produce simple, coherent image descriptions ‚Äî a beautiful example of merging computer vision with natural language processing.
        </p>
      </div>
    </section>
    </div>

    <script>
      const imageInput = document.getElementById("imageInput");
      const previewImage = document.getElementById("previewImage");
      const captionResult = document.getElementById("captionResult");
      const captionText = document.getElementById("captionText");
      const loadingOverlay = document.getElementById("loadingOverlay");

      imageInput.addEventListener("change", (e) => {
        const file = e.target.files[0];
        if (file) {
          const reader = new FileReader();
          reader.onload = function (event) {
            previewImage.src = event.target.result;
            previewImage.classList.remove("hidden");
          };
          reader.readAsDataURL(file);
        }
      });

      document
        .getElementById("uploadForm")
        .addEventListener("submit", async (e) => {
          e.preventDefault();

          const file = imageInput.files[0];
          if (!file) return;

          const formData = new FormData();
          formData.append("image", file);

          loadingOverlay.classList.remove("hidden");

          try {
            const response = await fetch("http://127.0.0.1:3000/caption", {
              method: "POST",
              body: formData,
            });

            const data = await response.json();
            captionText.textContent = data.caption || "No caption generated.";
            captionResult.classList.remove("hidden");

            confetti({
              particleCount: 150,
              spread: 70,
              origin: { y: 0.6 },
            });
          } catch (err) {
            captionText.textContent = "‚ùå Failed to get caption.";
            captionResult.classList.remove("hidden");
          } finally {
            loadingOverlay.classList.add("hidden");
          }
        });
    </script>
  </body>
</html>
